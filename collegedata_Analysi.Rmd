---
title: "R Notebook"
output:
  word_document: default
  html_notebook: default
  pdf_document: default
---

---
title: "Logistic Regression - Book Marketing"
output:
  pdf_document: default
  html_document:
    df_print: paged
  output: pdf_document
---


```{r}
library(tidyverse)
books <- read.csv("C:/Users/Amara Diallo/Desktop/SPRING 2019/Predictive 353/Exam/BookMarketing.csv")
str(books)
table(books$buyer_f)  # 45478 0's and 4522 1's
summary(books$buyer_f)  # mean = 0.09044 indicates proportion of 1's
sum(books$buyer_f)  # get 4522
0.09044*50000  # verifies 4522
```

 In this project we are acting as marketer of books. We are interested in pushing sales for a book called, “Art History of Florence.”Our data contains 50,000 customers and the const of mailing an offer is $2 and the profit from a sal of a book is $20. Our job is to find a way reduce expenses by maximizing our profit:

## Shotgun vs Rifle

 Shotgun Approach --> 
**We will Send offer to all 50,000 -->**

## Option 1 (Shotgun): This is what will happen if we Mail offer to everyone
-  Cost of offer = $2(50,000) = $100,000
- Response rate = 9.04% (i.e. 4522 buyers)
- Profit = $20(4522) = $90,440
- Net Profit = -$9,560

## Option 2 (Rifle): Mail offer to only those who will buy
- In this case, we do a test mailing on say 10,000 customers to develop the model. Model development includes selecting the best variables and using them to put together the regression equation.
- Next, the model is applied to the remaining 40,000 assuming the researcher is blind to the results. 
- Evaluate the costs and profits arising from misprediction in the model.

**Total net profit = 18*buyers + (-2)*nonbuyers -->**

```{r}
18*4522 + 45478*(-2)  # gives -9560.
45478+4522
4522/50000  # 0.09044
45478/50000  # 0.90956
```

## Strategy - pilot test on a subset
 We will Choose a random sample of 10,000 cases from the sample then perform an analysis on the selected 10,000 cases.Based on this, build a model that can help predict who will respond; After, we apply the results of selected sample to the remaining 40,000 cases to test how well the model fits ("Validation")

##Let's start Splitting our Data
 We will Randomly split this data by using function sample.split, this will divide data into 2 sets... The first set should have 20% of the data (training) and the second set (test data) will have 80% of the data. Sample.split will make sure similar propotion of buyers are in both the first and the second set (done by specifying books$buyer_f as the first argument)

```{r}
# Install an load caTools package
# install.packages("caTools")
library(caTools)  # for function sample.split

set.seed(99)
split <- sample.split(books$buyer_f, SplitRatio = 0.20)
split[1:20]

```

 Given a vector and a ratio value, yields a logical vector where the given ratio of the elements are assigned the value TRUE, and the remaining elements are assigned the value FALSE. This will Preserve relative ratios of categories in books$buyer_f , for instance, splits helps so that both subsets have similar proportion of bookbuyers.  

```{r}

summary(split)  # gives 40000 FALSE and 10000 TRUE
train <- subset(books, split ==1)  # equivalent to split == TRUE
test <- subset(books, split==0)
table(train$buyer_f)
```

 As we can see, we have 9096 0's and 904 1's  (0.0904 = proportion of buyers)  
Baseline model, for each observation, prob(buy) = 0.0904. We wil predict buy if prob(buy) > 0.5; So for each observation, predict not buy. We would guess for each person, they are not buyers... Number of actual non-buyers = 9096 
 For these people, our guess would be correct. However,for 904 buyers, our guess would not be correct.  
*overall accuracy = Number of correctly predicted/total Number observations* 
==> $$9096/10000 = 0.9096$$ 

```{r}
mean(train$buyer_f)  #0.0904
mean(test$buyer_f)  #0.09045
```

## Logistic Regression Model
In this section we will Run logistic regression in order to predict buyer_f based on all variables except for observation numbers (row numbers)

$$The below equation means that predictors are all variables except #buyer_f$$

```{r}
model <- glm(buyer_f ~ .-obs_num, data=train, family=binomial)
summary(model)
# model <- glm(buyer_f ~ ., data=train, family=binomial)
# This would mean predictors are all variables except #buyer_f

```

### syntax:  glm instead of lm, need to indicate family = binomial  
### Let's try all features except -obs_num

##cook_f has p value 0.781023, highest p value, therefore we will delete this

```{r}
model <- glm(buyer_f ~ .-obs_num -cook_f, data=train, family=binomial)
summary(model)

```

### pur3 has p value of 0.095197 (> 0.05), not bad, but we will try deleting since there are many other variables that have important p value.

```{r}
(model <- glm(buyer_f ~ .-obs_num -cook_f -pur3, data=train, family=binomial))
glm(buyer_f ~ .-obs_num -cook_f -pur3, data=train, family=binomial)
summary(model)
```

## Overall model significant?

Null deviance = deviance of baseline (no predictors, has only intercept term)  (sort of like SST)
deg of freedom = n - 1
Residual deviance = deviance of this model (sort of like SSE)
df = n - k - 1
Measure of improvement = chidiff = model$null.deviance - model$deviance
This has chi square distribution with deg of freedom = df of null - df of residual
= model$df.null - model$df.residual
Get p value with this:
pchisq(chidiff, dfdiff, lower.tail = F)

```{r}
(chidiff = model$null.deviance - model$deviance)
(dfdiff = model$df.null - model$df.residual)
pchisq(chidiff, dfdiff, lower.tail = F)

```

## Very small p value.  

### What is AIC? Can use to compare models
Akaike Information Criterion - this is like adj R^2
 But not like adj R^2, lower AIC is better

* Check for multicollinearity

```{r}
library(car)
vif(model)
```


vif > 5 - potential problem with multicollinearity, but no such problem here

## Make prediction

Fred:  l_pur=8, n_pur=3, pur3=1, pur7=2, cook_f=0, atlas_f=0, art_f=0

```{r}
model <- glm(buyer_f ~ gender + l_pur + n_pur + pur4 + pur5 + pur6 + pur7 + atlas_f + art_f, data=train, family=binomial)
fred <- data.frame(obs_num = 100, gender = 1, l_pur=8, n_pur=3, pur3=1, pur4 = 0, pur5 = 0, pur6 = 0, pur7=2, cook_f=0, atlas_f=0, art_f=0)
predict(model, type = "response", newdata = fred)
```
```{r}
model$coeff
model$coeff*c(1, 1, 8, 3, 0, 0, 0, 2, 0, 0)
z <- sum(model$coeff*c(1, 1, 8, 3, 0, 0, 0, 2, 0, 0))
prob <- 1/(1+exp(-z))
(predict(model, newdata = fred))  # this gives the logit too
```

```{r}
(odds <- prob/(1-prob))
```

Bob same as Fred but has purchased "Italian Art"  
Coeff here is 0.568  
Bob's z = Fred's z + 0.568  
Recall z = ln of odds  
Odds = e^z  
Bob's odds = exp(Fred's z + 0.568) = Fred's odds * exp(0.568)  
= 0.405*exp(0.568)

```{r}
exp(0.568051)
odds*exp(0.568051)


```

### direct calculation

```{r}
bob <- data.frame(gender = 1, l_pur=8, n_pur=3, pur3=1, pur4 = 0, pur5 = 0, pur6 = 0, pur7=2, cook_f=0, atlas_f=0, art_f=1)
(prob <- predict(model, type = "response", newdata = bob))
(odds <- prob/(1-prob))
```

$$ We~ will~ Show~ first~ 10~ probabilities $$

for each person whose prob. > 0.5,we will  classify them as potential buyer

```{r}
model$fitted.values[1:10]  # Show first 10 probabilities
predictTrain <- model$fitted.values
# or alternatively, we can do
# predictTrain <- predict(model, type = "response") 
table(train$buyer_f, predictTrain > 0.5)  
     # for each person whose prob. > 0.5, classify them as buyer
```

## This is a classification matrix (also called confusion matrix).  
The rows correspond to observed counts ("actual") and the columns correspond to predicted counts resulting from the model.

```{r}
(9020+185)/10000  # 0.9205 = overall accuracy  (increased from 0.9096)
9020+76   # actual 0's
719+185   # actual 1's
185/904    # tpr = true positive/actual positive = 0.205  (called sensitivity)
9020/9096  # tnr = true negative/actual negative = 0.992  (called specificity)
1-9020/9096  # fpr = false positive/actual negative = 0.0084
```

- Overall accuracy = proportion of sample that is correctly classified  
- Sensitivity = Number true positives / Number actual positives = proportion of buyers that are correctly identified  
- Specificity = Number true negatives / Number actual negatives = proportion of non-buyers that are correctly identified  
Hit rate (in marketing) = Number of true positives / Number predicted positives = proportion of predicted buyers that are actual buyers  

## What if threshold is higher? (e.g., classify as buyer only if prob > 0.7)
fewer predicted positive, more predicted negative 

## Let's Make prediction on the test data (of 40,000 observations)
This will gives probabilities for test data with the model built on train data

```{r}
predictTest <- predict(model, type = "response", newdata=test)  
# gives probabilities for test data with the model built on train data

```

### We will Build a classification matrix with the prediction on the test data
$$ The below equation is the number of predicted to buy & actually buy (True positives)$$

```{r}
table(test$buyer_f, predictTest > 0.5)
sum(predictTest > 0.5)  # predicted to buy
sum(test$buyer_f)  # actual buyers
sum(predictTest > 0.5 & test$buyer_f)  # predicted to buy & actually buy (True positives)
```

## Economic Analysis  
Total net profit: Profit from training data of 10,000 + profit from testing data of 40,000

```{r}

profit1 <- 20*sum(train$buyer_f) # profit from buyers in the training group
cost1 <- 2*nrow(train)  # cost of sending offer to everyone in the training group
predBuyer <- predictTest > 0.4  
  # predBuyer is logical vector of who are predicted to buy from the test group
cost2 <- 2*sum(predBuyer)  # cost of sending offer to those predicted to buy
profit2 <- 20*sum(predBuyer & test$buyer_f)  
                     # profit from the actual buyers who were predicted to buy
(totalNet <- profit1 - cost1 + profit2 - cost2)  # ( ) lets  print the resulting totalNet value

```

Lowering the threshold from 0.5 seems to yield higher total net profit. We will Calculate total net profit for different threshold values:  0.5, 0.49, 0.48, ..., 0.05. Use *record* to store the results.


```{r}

record <- data.frame(Threshold = numeric(), TotalProfit = numeric())
for (t in seq(0.5, 0.05, -0.01)) {
  profit1 <- 20*sum(train$buyer_f) 
  cost1 <- 2*nrow(train)  
  predBuyer <- predictTest > t  
  cost2 <- 2*sum(predBuyer)  
  profit2 <- 20*sum(predBuyer & test$buyer_f)  
  totalNet <- profit1 - cost1 + profit2 - cost2
  record <- rbind(record, data.frame(Threshold = t, TotalProfit = totalNet))
}
plot(record$Threshold, record$TotalProfit)
```


 $$ identify~the ~threshold ~with ~highest ~net ~profit $$


```{r}
record[which.max(record$TotalProfit),]  # identify the threshold with highest net profit

```


The optimal threshold is 0.12 with maximum total profit of $27,562. This says send the offer to the customers whose probability of purchase is higher than 0.12.


## Receiver Operator Characteristic (ROC) Curve 
ROC curve can be used to (1) select a good threshold value and to (2) measure model performance

```{r}
table(train$buyer_f, predictTrain > 0.7)
9077+87  # 9164 = # correct predictions, overall accuracy = 0.9164
87/(817+87) # tpr = 0.0962
19/(9077+19)
```

- false positive = 19  
- actual negative = 9077+19  

0.0021  

- Increasing tpr leads to increasing fpr  
## What is the acceptable threshold?  
Plot tpr and fpr for various threshold values  

```{r}
#install.packages("ROCR")
library(ROCR)
ROCRpred <- prediction(predictTrain, train$buyer_f)
```

 Given a vector prob's (predictTrain) and label of 1's and 0's (train$buyer_f), makes prediction for various values of thresholds

```{r}
ROCRperf <- performance(ROCRpred, "tpr", "fpr")
plot(ROCRperf, colorize=TRUE, print.cutoffs.at=seq(0, 1, by=0.1), text.adj=c(-0.2,1.7)) 
abline(a=0, b=1)
```

text.adj:  first value - horizontal adj (neg = right), second value - vertical adj (neg = above)

 If threshold t = 0, every observation will satisfy predTrain > 0, so every observation will be classfied as 1.  Then tpr = 1 and fpr = 1.  As t is increased from 0 to 1, both tpr and fpr decrease.   
### Choose t where tpr is relatively high with fpr acceptably low.

```{r}
performance(ROCRpred, measure = "auc")@y.values
```


 0.8343852 = area under the curve
 This measures the performance of the model, closer to 1 the better.  Minimum is 0.5 - this is equivalent to random guessing.
